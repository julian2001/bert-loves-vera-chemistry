# -*- coding: utf-8 -*-
"""dmpnn_model2-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ypszTbnMeZBB3bn-5BNfNuzkgkYaT6iX
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

!pip install --pre deepchem
!pip install torch_geometric #required for DMPNN model
!pip install triton  #required for reduce-overhead mode
!pip install --upgrade torch
!pip install --upgrade deepchem
!pip install rdkit biopython deepchem
!pip install os

import torch

import datetime
import numpy as np
import deepchem as dc

import matplotlib.pyplot as plt

torch._dynamo.config.cache_size_limit = 1000000000

import deepchem as dc
import os
from rdkit import Chem

# Directories
pdb_dir = '/content/drive/My Drive/drug_discovery_datasets/pdb_ligand_folder'
sdf_dir = '/content/drive/My Drive/drug_discovery_datasets/sdf_converted_files3'
smiles_file = '/content/drive/My Drive/drug_discovery_datasets/gbd_17_dataset_verified_non_canonicalized'  # Assuming CSV with 'smiles' column

# Function to load SMILES strings
def load_smiles(smiles_file):
    """Reads SMILES strings from a CSV file."""
    smiles_list = []
    if os.path.exists(smiles_file):
        import pandas as pd
        smiles_df = pd.read_csv(smiles_file)
        smiles_list = smiles_df['smiles'].tolist()
    return smiles_list

# Function to load SDF files
def load_sdf_files(sdf_dir):
    """Loads molecules from SDF files in the directory."""
    sdf_mols = []
    for sdf_file in os.listdir(sdf_dir):
        if sdf_file.endswith('.sdf'):
            suppl = Chem.SDMolSupplier(os.path.join(sdf_dir, sdf_file))
            sdf_mols.extend([mol for mol in suppl if mol is not None])
    return sdf_mols

# Function to load PDB files
def load_pdb_files(pdb_dir):
    """Loads molecules from PDB files in the directory."""
    pdb_mols = []
    for pdb_file in os.listdir(pdb_dir):
        if pdb_file.endswith('.pdb'):
            mol = Chem.MolFromPDBFile(os.path.join(pdb_dir, pdb_file))
            if mol is not None:
                pdb_mols.append(mol)
    return pdb_mols

# Combine all molecules from SMILES, SDF, and PDB
def load_all_molecules(pdb_dir, sdf_dir, smiles_file):
    """Loads and combines molecules from SMILES, SDF, and PDB files."""
    all_molecules = []

    # Load SMILES
    smiles_list = load_smiles(smiles_file)
    smiles_mols = [Chem.MolFromSmiles(smile) for smile in smiles_list if Chem.MolFromSmiles(smile) is not None]
    all_molecules.extend(smiles_mols)

    # Load SDF molecules
    sdf_molecules = load_sdf_files(sdf_dir)
    all_molecules.extend(sdf_molecules)

    # Load PDB molecules
    pdb_molecules = load_pdb_files(pdb_dir)
    all_molecules.extend(pdb_molecules)

    return all_molecules

# Load molecules
all_molecules = load_all_molecules(pdb_dir, sdf_dir, smiles_file)
print(f"Loaded {len(all_molecules)} molecules from PDB, SDF, and SMILES")

# Use DeepChem's DMPNNFeaturizer to featurize molecules
featurizer = dc.feat.DMPNNFeaturizer()

# Featurize molecules into the input required for DMPNN model
features = featurizer.featurize(all_molecules)
print(f"Featurized {len(features)} molecules")

# Create dummy tasks (you will replace with real tasks and dataset later)
tasks = ['dummy_task']
dataset = dc.data.NumpyDataset(X=features, y=None, ids=None)

# Split the dataset (using random splitting for simplicity)
splitter = dc.splits.RandomSplitter()
train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset)

print(f"Training data size: {len(train_dataset)}")
print(f"Validation data size: {len(valid_dataset)}")
print(f"Test data size: {len(test_dataset)}")

# Initialize and train DMPNN model
model = dc.models.DMPNNModel(n_tasks=len(tasks))
model.fit(train_dataset, nb_epoch=10)

!nvidia-smi

model.compile()

model.fit(train_dataset, nb_epoch=10)

metrics = [dc.metrics.Metric(dc.metrics.mean_squared_error)]
print(f"Training MSE: {model.evaluate(train_dataset, metrics=metrics)}")
print(f"Validation MSE: {model.evaluate(valid_dataset, metrics=metrics)}")
print(f"Test MSE: {model.evaluate(test_dataset, metrics=metrics)}")

def time_torch_function(fn):
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    start.record()
    result = fn()
    end.record()
    torch.cuda.synchronize()
    return result, start.elapsed_time(end) / 1000

track_dict = {}
prev_time_dict = {}

def get_time_track_callback(track_dict, track_name, track_interval):
    track_dict[track_name] = []
    prev_time_dict[track_name] = datetime.datetime.now()
    def callback(model, step):
        if step % track_interval == 0:
            elapsed_time = datetime.datetime.now() - prev_time_dict[track_name]
            track_dict[track_name].append(elapsed_time.total_seconds())
            prev_time_dict[track_name] = datetime.datetime.now()
    return callback

model = dc.models.DMPNNModel()
model_compiled = dc.models.DMPNNModel()  # No mode='reduce-overhead'

track_interval = 20
eager_dict_name = "eager_train"
compiled_dict_name = "compiled_train"

eager_train_callback = get_time_track_callback(track_dict, eager_dict_name, track_interval)
model.fit(train_dataset, nb_epoch=10, callbacks=[eager_train_callback])

compiled_train_callback = get_time_track_callback(track_dict, compiled_dict_name, track_interval)
model_compiled.fit(train_dataset, nb_epoch=10, callbacks=[compiled_train_callback])

eager_train_times = track_dict[eager_dict_name]
compiled_train_times = track_dict[compiled_dict_name]

print(f"Eager Times (first 15): {[f'{t:.3f}' for t in eager_train_times[:15]]}")
print(f"Compiled Times (first 15): {[f'{t:.3f}' for t in compiled_train_times[:15]]}")
print(f"Total Eager Time: {sum(eager_train_times)}")
print(f"Total Compiled Time: {sum(compiled_train_times)}")
print(f"Eager Median: {np.median(eager_train_times)}")
print(f"Compiled Median: {np.median(compiled_train_times)}")
print(f"Median Speedup: {((np.median(eager_train_times) / np.median(compiled_train_times)) - 1) * 100:.2f}%")

x_vals = np.arange(1, len(eager_train_times) + 1) * track_interval
plt.plot(x_vals, eager_train_times, label="Eager")
plt.plot(x_vals, compiled_train_times, label="Compiled")
plt.yscale('log', base= 10)
plt.ylabel('Time (s)')
plt.xlabel('Batch Iteration')
plt.legend()
plt.show()

model = dc.models.DMPNNModel()
model_compiled = dc.models.DMPNNModel()
model_compiled.compile(mode='reduce-overhead')

iters = 100

eager_predict_times = []
compiled_predict_times = []

for i in range(iters):
    for X, y, w, ids in test_dataset.iterbatches(64, pad_batches=True):
        with torch.no_grad():
            _, eager_time = time_torch_function(lambda: model.predict_on_batch(X))
            _, compiled_time = time_torch_function(lambda: model_compiled.predict_on_batch(X))

        eager_predict_times.append(eager_time)
        compiled_predict_times.append(compiled_time)

print(f"Eager Times (first 15): {[f'{t:.3f}' for t in eager_predict_times[:15]]}")
print(f"Compiled Times (first 15): {[f'{t:.3f}' for t in compiled_predict_times[:15]]}")
print(f"Total Eager Time: {sum(eager_predict_times)}")
print(f"Total Compiled Time: {sum(compiled_predict_times)}")
print(f"Eager Median: {np.median(eager_predict_times)}")
print(f"Compiled Median: {np.median(compiled_predict_times)}")
print(f"Median Speedup: {((np.median(eager_predict_times) / np.median(compiled_predict_times)) - 1) * 100:.2f}%")

plt.plot(eager_predict_times, label= "Eager")
plt.plot(compiled_predict_times, label= "Compiled")
plt.ylabel('Time (s)')
plt.xlabel('Batch Iteration')
plt.yscale('log', base= 10)
plt.legend()

import deepchem as dc
import os
import pickle
import logging
import traceback
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from rdkit import Chem

# Set up directories
log_dir = '/content/drive/My Drive/drug_discovery_datasets/logs'
os.makedirs(log_dir, exist_ok=True)  # Ensure log directory exists

save_dir = '/content/drive/My Drive/drug_discovery_datasets/dmpnn_evaluation_results'
os.makedirs(save_dir, exist_ok=True)  # Ensure results directory exists

# Set up logging
logging.basicConfig(filename=os.path.join(log_dir, 'evaluation_errors.log'),
                    level=logging.ERROR,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Log error function to handle logging of error messages
def log_error(message):
    """Logs an error message to the evaluation_errors.log file."""
    logging.error(message)

# Function to save results
def save_results(eager_times, compiled_times, save_dir):
    """Save eager and compiled benchmark results as .pkl and molecular structures as .xyz and .pdb."""

    # Save eager and compiled times to a pickle file
    results = {
        "eager_times": eager_times,
        "compiled_times": compiled_times
    }
    try:
        with open(os.path.join(save_dir, 'dmpnn_benchmark_results.pkl'), 'wb') as f:
            pickle.dump(results, f)
        print("Benchmark results saved to pickle file successfully.")
    except Exception as e:
        log_error(f"Error saving pickle file: {e}")
        traceback.print_exc()

    # Generate and save a sample molecule as .xyz and .pdb using RDKit
    try:
        mol = Chem.MolFromSmiles('CCO')  # Simple ethanol molecule as an example
        mol_block = Chem.MolToMolBlock(mol)

        # Save in XYZ format
        xyz_path = os.path.join(save_dir, 'sample_molecule.xyz')
        with open(xyz_path, 'w') as xyz_file:
            xyz_file.write(mol_block)
        print(f"Sample molecule saved as XYZ at {xyz_path}")

        # Save in PDB format
        pdb_path = os.path.join(save_dir, 'sample_molecule.pdb')
        Chem.MolToPDBFile(mol, pdb_path)
        print(f"Sample molecule saved as PDB at {pdb_path}")

    except Exception as e:
        log_error(f"Error saving molecular structure files: {e}")
        traceback.print_exc()

# Call the save_results function after plotting
save_results(eager_predict_times, compiled_predict_times, save_dir)